{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/haoweiwang0/Coreset_Prioritization.git\n",
    "%cd Coreset_Prioritization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install neural-tangents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"jax[cuda12]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bilevel_coreset\n",
    "import loss_utils\n",
    "import ntk_generator\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import torch.nn.functional as F\n",
    "\n",
    "subset_size = 50\n",
    "\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "same_seeds(0)\n",
    "\n",
    "mnist_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "train_dataset = datasets.MNIST(root='data', train=True, transform=mnist_transforms, download=True)\n",
    "test_dataset = datasets.MNIST('data', train=False, transform=mnist_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256cea58ae7a443d93d724f31c98b574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/50 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a4f01af95f4cdc873165f157c2bbbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/469 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform sample - Train accuracy 0.99365\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97ecbe611f74a2487dca11358fc9e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/79 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform sample - Test accuracy 0.9914\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import models\n",
    "\n",
    "def train_model(model, loader):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=0)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    pbar = tqdm(range(nr_epochs), desc=\"Training\", unit=\"epoch\")\n",
    "\n",
    "    for epoch in pbar:\n",
    "        model.train()\n",
    "        training_loss = 0.0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            training_loss += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "        pbar.set_postfix({'loss': training_loss / (batch_idx + 1)})\n",
    "\n",
    "def test_model(model, loader):\n",
    "    model.to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.eval()\n",
    "    pbar = tqdm(loader, desc=\"Testing\", unit=\"batch\")\n",
    "    correct = 0\n",
    "    testing_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in pbar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            testing_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            pbar.set_postfix({'loss': testing_loss / len(loader)})\n",
    "    test_acc = 1. * correct / len(loader.dataset)\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_classes = 10\n",
    "batch_size = 128\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "nr_epochs = 50\n",
    "\n",
    "resnet18 = models.ResNet18().to(device)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "train_model(resnet18, train_loader)\n",
    "print('Original ResNet - Train accuracy', test_model(resnet18, train_loader))\n",
    "print('Original ResNet - Test accuracy', test_model(resnet18, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_kernel_fn = lambda x, y: ntk_generator.generate_resnet_ntk(x.view(-1, 28, 28, 1).numpy(), y.view(-1, 28, 28, 1).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work only on the first 10000 samples for speedup\n",
    "\n",
    "limit = 10000\n",
    "\n",
    "loader = torch.utils.data.DataLoader(train_dataset, batch_size=limit, shuffle=False)\n",
    "X, y = next(iter(loader))\n",
    "\n",
    "bc = bilevel_coreset.BilevelCoreset(outer_loss_fn=loss_utils.cross_entropy,\n",
    "                                    inner_loss_fn=loss_utils.cross_entropy, out_dim=10,\n",
    "                                    max_outer_it=10, outer_lr=0.05, max_inner_it=200)\n",
    "coreset_inds, _ = bc.build_with_representer_proxy_batch(X, y, subset_size, proxy_kernel_fn, cache_kernel=True,\n",
    "                                                        start_size=10, inner_reg=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_epochs = 1000\n",
    "\n",
    "coreset_net = models.ResNet18().to(device)\n",
    "coreset_subset = Subset(train_dataset, coreset_inds)\n",
    "train_loader = torch.utils.data.DataLoader(coreset_subset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "train_model(coreset_net, train_loader)\n",
    "print('Coreset ResNet - Train accuracy', test_model(coreset_net, train_loader))\n",
    "print('Coreset ResNet - Test accuracy', test_model(coreset_net, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_logits(model, coreset_model, loader):\n",
    "    model.eval()\n",
    "    logits = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = F.softmax(model(data), dim=1)\n",
    "            coreset_output = F.softmax(coreset_model(data), dim=1)\n",
    "            logits.append((data, output, coreset_output, target))\n",
    "\n",
    "    return logits\n",
    "\n",
    "logits = test_with_logits(resnet18, coreset_net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prioritized_test_data = []\n",
    "\n",
    "# calculate the similarity score between the logits from\n",
    "# original DNN and coreset-trained DNN\n",
    "# reorder test cases based on the similarity score\n",
    "def test_case_prioritization(logits):\n",
    "    result = []\n",
    "    probs = torch.cat([item[1] for item in logits], dim=0).cpu().numpy()\n",
    "    coreset_probs = torch.cat([item[2] for item in logits], dim=0).cpu().numpy()\n",
    "    inputs = torch.cat([item[0] for item in logits], dim=0).cpu().numpy()\n",
    "    targets = torch.cat([item[3] for item in logits], dim=0).cpu().numpy()\n",
    "    for i in range(probs.shape[0]):\n",
    "        item = probs[i]\n",
    "        coreset_item = coreset_probs[i]\n",
    "        similarity_score = F.cosine_similarity(torch.tensor(item).unsqueeze(0), torch.tensor(coreset_item).unsqueeze(0))\n",
    "        result.append({'similarity_score': similarity_score,\n",
    "                       'probabilities': item,\n",
    "                       'input': inputs[i],\n",
    "                       'target': targets[i]})\n",
    "    result = sorted(result, key=lambda x: x['similarity_score'], reverse=False)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prioritized_test_data = test_case_prioritization(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "inputs = torch.stack([torch.from_numpy(item['input']) for item in prioritized_test_data])\n",
    "targets = torch.stack([torch.tensor(item['target']) for item in prioritized_test_data])\n",
    "tensor_dataset = TensorDataset(inputs, targets)\n",
    "prioritized_loader = torch.utils.data.DataLoader(tensor_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_APFD(model, loader):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    TFs = 10 * [0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ids, (data, target) in enumerate(loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            category = output.argmax(dim=1, keepdim=True)\n",
    "            if category != target and TFs[target] == 0:\n",
    "                TFs[target] = ids + 1\n",
    "\n",
    "            if all(TFs):\n",
    "                break\n",
    "\n",
    "    APFD = 1 - (sum(TFs) / (10 * len(loader))) + 1 / (2 * len(loader))\n",
    "\n",
    "    return APFD\n",
    "\n",
    "print('ResNet - APFD', test_with_APFD(resnet18, prioritized_loader))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
